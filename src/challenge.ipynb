{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/blanc/Documents/challenge_DE/data/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer desafío\n",
    "\n",
    "## Introducción y estrategia\n",
    "\n",
    "El objetivo de esta tarea es obtener las diez fechas con más twits y sus respectivos usuarios con mayor cantidad de twits en ellas. \n",
    "\n",
    "Mi primer enfoque en esta tarea fue pensar en algún paquete que fuera óptimo para procesar datos y por defecto descarté `pandas`, ya que a pesar de ser muy cómodo y útil para el análisis de datos, no se destaca por su eficiencia. Por tanto mi primera opción fue `polars`. Mi primer código se veía más o menos así:\n",
    "\n",
    "```\n",
    "df_scan = pl.scan_ndjson(file_path, infer_schema_length=None)\n",
    "\n",
    "df_output = df_scan \\\n",
    "    .select(\n",
    "        [\n",
    "            pl.col('date').str.strptime(pl.Datetime).dt.date().alias('datetime'),\n",
    "            pl.col('user').struct.field('username').alias('username')\n",
    "        ]\n",
    "    ) \\\n",
    "    .group_by(['datetime', 'username']) \\\n",
    "    .agg(\n",
    "        twits_count = pl.count('username')\n",
    "    ) \\\n",
    "    .group_by('datetime') \\\n",
    "    .agg([\n",
    "        pl.col('username').gather(pl.col('twits_count').arg_max()),\n",
    "        pl.col('twits_count').gather(pl.col('twits_count').arg_max())\n",
    "    ]) \\\n",
    "    .with_columns(\n",
    "        pl.col('username').list.first(),\n",
    "        pl.col('twits_count').list.first()\n",
    "    ) \\\n",
    "    .sort('twits_count', descending=True) \\\n",
    "    .select(['datetime', 'username']) \\\n",
    "    .head(10)\n",
    "```\n",
    "\n",
    "Tan pronto comprobé que estas líneas cumplían el objetivo, me dispuse a buscar otro método para optimizar el uso de memoria, ya que `polars` utiliza procesamiento en paralelo y si bien seguramente era muy rápido el procesamiento, exigía mucho en términos de memoria. Me imaginé que un enfoque secuencial, recorriendo el archivo json línea a línea sería más eficiente aunque quizás más lento. Sin embargo, pronto comprobé que este método no solo era mejor en términos de uso de memoria, sino también de velocidad, utilizando tan solo un 33% del tiempo que `polars` requería para completar la tarea. Es entonces que llegué a la función `q1_memory`.\n",
    "\n",
    "La estructura de la función es la siguiente:\n",
    "\n",
    "1. Genera una instancia de contador, utilizando la clase de python Counter\n",
    "2. Recorre línea por línea el archivo json almacenando en el contador la cantidad de menciones para cada fecha-usuario.\n",
    "3. Mediante la función `most_common`, obtenemos el usuario más repetido por fecha, y lo guardamos en un array `results`.\n",
    "4. Ordenamos el array `results` de forma descendiente por la \"columna\" `count` y limitados el resultado a 10 elementos. \n",
    "5. Finalmente, retornamos el resultado.\n",
    "\n",
    "```\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    user_counts_per_date = defaultdict(Counter)\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            date = datetime.strptime(record['date'], '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "            username = record['user']['username']\n",
    "            user_counts_per_date[date][username] += 1\n",
    "\n",
    "    results = []\n",
    "    for date, counter in user_counts_per_date.items():\n",
    "        most_common_user, count = counter.most_common(1)[0]\n",
    "        results.append((date, most_common_user, count))\n",
    "\n",
    "    top_results = sorted(results, key=lambda x: (-x[2], x[0]))[:10]\n",
    "    return [(date, username) for date, username, _ in top_results]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendimiento de `q1_memory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para analizar el rendimiento de las funciones, se les ha colocado el decorador `@track_execution_time`, que es una función auxiliar que busca trackear el tiempo de ejecución de cada una. Además, sumaremos el uso de `memory_profiler` para monitorear el consumo de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de función q1_memory\n",
    "from q1_memory import q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_memory executed in 2.4085888862609863 seconds.\n",
      "peak memory: 72.28 MiB, increment: 3.83 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit # Usamos este magic command para aplicar el memory_profiler en esta celda\n",
    "\n",
    "results_q1_memory = q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede apreciarse, el tiempo de ejecución es de 2.4 segundos, generando un incremento de 3.8 megas de ram, ubicándonos en un pico de 72.3 en total. A continuación, los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 21), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_q1_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendimiento de `q1_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como mencioné anteriormente, era tal la diferencia tanto en uso de memoria como en velocidad de ejecución entre el enfoque secuencial de json y el uso de `polars`, que me hizo cuestionarme el utilizar `polars` como tal. Entendí que la fortaleza de `polars` radicaba en el uso de procesamiento paralelo, pero al parecer esto no llegaba a justificar su uso. Por tanto me propuse mantener la estructura de `q1_memory` pero agregándole procesamiento paralelo, buscando obtener el mismo beneficio que `polars` pero sin tener que utilizar tan pesada librería. Es así que di con el paquete `concurrency`, que me permitió desarrollar la misma estrategía que `q1_memory` pero de forma paralela.\n",
    "\n",
    "La estructura de la función es la siguiente:\n",
    "1. Definimos `process_lines`, que básicamente realiza el trabajo de contar lineas por fecha-usuario, tal como en la otra función.\n",
    "2. Generamos una serie de chunks sobre el dataset original, de modo de procesar esta información paralelamente más adelante.\n",
    "3. Generamos una instancia de `ThreadPoolExecutor`, en donde se aplicará la función `process_lines` a cada chunk creado\n",
    "4. En la medida que los chunks se van procesando, se agrupan en el array `results`\n",
    "5. Mediante la función `most_common`, obtenemos el usuario más repetido por fecha, y lo guardamos en un array `final_results`.\n",
    "6. Ordenamos el array `results` de forma descendiente por la \"columna\" `count` y limitados el resultado a 10 elementos. \n",
    "7. Finalmente, retornamos el resultado.\n",
    "\n",
    "Como se habrá notado, la mayoría de los pasos son exactamente igual que en la función q1_memory, con una única adición de procesamiento paralelo para mayor velocidad.\n",
    "\n",
    "\n",
    "```\n",
    "def process_lines(lines):\n",
    "    local_counts = defaultdict(Counter)\n",
    "    for line in lines:\n",
    "        record = json.loads(line)\n",
    "        date = datetime.strptime(record['date'], '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        username = record['user']['username']\n",
    "        local_counts[date][username] += 1\n",
    "    return local_counts\n",
    "\n",
    "@track_execution_time\n",
    "def q1_time(file_path: str, lines_per_chunk = 1000, num_workers = 4) -> List[Tuple[datetime.date, str]]:\n",
    "    \n",
    "    chunks = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            chunk.append(line)\n",
    "            if len(chunk) >= lines_per_chunk:\n",
    "                chunks.append(chunk)\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    results = defaultdict(Counter)\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(process_lines, chunk) for chunk in chunks]\n",
    "        for future in as_completed(futures):\n",
    "            local_counts = future.result()\n",
    "            for date, counter in local_counts.items():\n",
    "                results[date] += counter\n",
    "\n",
    "    final_results = []\n",
    "    for date, counter in results.items():\n",
    "        most_common_user, count = counter.most_common(1)[0]\n",
    "        final_results.append((date, most_common_user, count))\n",
    "\n",
    "    top_results = sorted(final_results, key=lambda x: (-x[2], x[0]))[:10]\n",
    "    return [(date, username) for date, username, _ in top_results]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de función q1_time\n",
    "from q1_time import q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_time executed in 2.806195020675659 seconds.\n",
      "peak memory: 503.52 MiB, increment: 442.39 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "results_q1_time = q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de que teóricamente el objetivo de utilizar este método es optimizar el tiempo de ejecución, ese objetivo no se cumple en este caso. Mi hipótesis para este resultado es que el tamaño del dataset no permite brillar la utilidad del procesamiento paralelo. En la medida que esta tarea se aplique a un dataset de mayor tamaño, es donde el enfoque secuencial comenzará a quedarse atrás en términos de tiempo de ejecución en comparación al enfoque paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 21), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
