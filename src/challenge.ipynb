{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/blanc/Documents/challenge_DE/data/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer desafío\n",
    "\n",
    "## Introducción y estrategia\n",
    "\n",
    "El objetivo de esta tarea es obtener las diez fechas con más twits y sus respectivos usuarios con mayor cantidad de twits en ellas. \n",
    "\n",
    "Mi primer enfoque en esta tarea fue pensar en algún paquete que fuera óptimo para procesar datos y por defecto descarté `pandas`, ya que a pesar de ser muy cómodo y útil para el análisis de datos, no se destaca por su eficiencia. Por tanto mi primera opción fue `polars`. Mi primer código se veía más o menos así:\n",
    "\n",
    "```\n",
    "df_scan = pl.scan_ndjson(file_path, infer_schema_length=None)\n",
    "\n",
    "df_output = df_scan \\\n",
    "    .select(\n",
    "        [\n",
    "            pl.col('date').str.strptime(pl.Datetime).dt.date().alias('datetime'),\n",
    "            pl.col('user').struct.field('username').alias('username')\n",
    "        ]\n",
    "    ) \\\n",
    "    .group_by(['datetime', 'username']) \\\n",
    "    .agg(\n",
    "        twits_count = pl.count('username')\n",
    "    ) \\\n",
    "    .group_by('datetime') \\\n",
    "    .agg([\n",
    "        pl.col('username').gather(pl.col('twits_count').arg_max()),\n",
    "        pl.col('twits_count').gather(pl.col('twits_count').arg_max())\n",
    "    ]) \\\n",
    "    .with_columns(\n",
    "        pl.col('username').list.first(),\n",
    "        pl.col('twits_count').list.first()\n",
    "    ) \\\n",
    "    .sort('twits_count', descending=True) \\\n",
    "    .select(['datetime', 'username']) \\\n",
    "    .head(10)\n",
    "```\n",
    "\n",
    "Tan pronto comprobé que estas líneas cumplían el objetivo, me dispuse a buscar otro método para optimizar el uso de memoria, ya que `polars` utiliza procesamiento en paralelo y si bien seguramente era muy rápido el procesamiento, exigía mucho en términos de memoria. Me imaginé que un enfoque secuencial, recorriendo el archivo json línea a línea sería más eficiente aunque quizás más lento. Sin embargo, pronto comprobé que este método no solo era mejor en términos de uso de memoria, sino también de velocidad, utilizando tan solo un 33% del tiempo que `polars` requería para completar la tarea. Es entonces que llegué a la función `q1_memory`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendimiento de `q1_memory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estructura de la función es la siguiente:\n",
    "\n",
    "1. Genera una instancia de contador, utilizando la clase de python Counter\n",
    "2. Recorre línea por línea el archivo json almacenando en el contador la cantidad de menciones para cada fecha-usuario.\n",
    "3. Mediante la función `most_common`, obtenemos el usuario más repetido por fecha, y lo guardamos en un array `results`.\n",
    "4. Ordenamos el array `results` de forma descendiente por la \"columna\" `count` y limitados el resultado a 10 elementos. \n",
    "5. Finalmente, retornamos el resultado.\n",
    "\n",
    "```\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    user_counts_per_date = defaultdict(Counter)\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            date = datetime.strptime(record['date'], '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "            username = record['user']['username']\n",
    "            user_counts_per_date[date][username] += 1\n",
    "\n",
    "    results = []\n",
    "    for date, counter in user_counts_per_date.items():\n",
    "        most_common_user, count = counter.most_common(1)[0]\n",
    "        results.append((date, most_common_user, count))\n",
    "\n",
    "    top_results = sorted(results, key=lambda x: (-x[2], x[0]))[:10]\n",
    "    return [(date, username) for date, username, _ in top_results]\n",
    "```\n",
    "\n",
    "Para analizar el rendimiento de las funciones, se les ha colocado el decorador `@track_execution_time`, que es una función auxiliar que busca trackear el tiempo de ejecución de cada una. Además, sumaremos el uso de `memory_profiler` para monitorear el consumo de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de función q1_memory\n",
    "from q1_memory import q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_memory executed in 2.4085888862609863 seconds.\n",
      "peak memory: 72.28 MiB, increment: 3.83 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit # Usamos este magic command para aplicar el memory_profiler en esta celda\n",
    "\n",
    "results_q1_memory = q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede apreciarse, el tiempo de ejecución es de 2.4 segundos, generando un incremento de 3.8 megas de ram, ubicándonos en un pico de 72.3 en total. A continuación, los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 21), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de función q1_memory\n",
    "from q1_memory import q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_memory executed in 2.4085888862609863 seconds.\n",
      "peak memory: 72.28 MiB, increment: 3.83 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit # Usamos este magic command para aplicar el memory_profiler en esta celda\n",
    "\n",
    "results_q1_memory = q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede apreciarse, el tiempo de ejecución es de 2.4 segundos, generando un incremento de 3.8 megas de ram, ubicándonos en un pico de 72.3 en total. A continuación, los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 21), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_q1_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendimiento de `q1_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como mencioné anteriormente, era tal la diferencia tanto en uso de memoria como en velocidad de ejecución entre el enfoque secuencial de json y el uso de `polars`, que me hizo cuestionarme el utilizar `polars` como tal. Entendí que la fortaleza de `polars` radicaba en el uso de procesamiento paralelo, pero al parecer esto no llegaba a justificar su uso. Por tanto me propuse mantener la estructura de `q1_memory` pero agregándole procesamiento paralelo, buscando obtener el mismo beneficio que `polars` pero sin tener que utilizar tan pesada librería. Es así que di con el paquete `concurrency`, que me permitió desarrollar la misma estrategía que `q1_memory` pero de forma paralela.\n",
    "\n",
    "La estructura de la función es la siguiente:\n",
    "1. Definimos `process_lines`, que básicamente realiza el trabajo de contar lineas por fecha-usuario, tal como en la otra función.\n",
    "2. Generamos una serie de chunks sobre el dataset original, de modo de procesar esta información paralelamente más adelante.\n",
    "3. Generamos una instancia de `ThreadPoolExecutor`, en donde se aplicará la función `process_lines` a cada chunk creado\n",
    "4. En la medida que los chunks se van procesando, se agrupan en el array `results`\n",
    "5. Mediante la función `most_common`, obtenemos el usuario más repetido por fecha, y lo guardamos en un array `final_results`.\n",
    "6. Ordenamos el array `results` de forma descendiente por la \"columna\" `count` y limitados el resultado a 10 elementos. \n",
    "7. Finalmente, retornamos el resultado.\n",
    "\n",
    "Como se habrá notado, la mayoría de los pasos son exactamente igual que en la función q1_memory, con una única adición de procesamiento paralelo para mayor velocidad.\n",
    "\n",
    "\n",
    "```\n",
    "def process_lines(lines):\n",
    "    local_counts = defaultdict(Counter)\n",
    "    for line in lines:\n",
    "        record = json.loads(line)\n",
    "        date = datetime.strptime(record['date'], '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        username = record['user']['username']\n",
    "        local_counts[date][username] += 1\n",
    "    return local_counts\n",
    "\n",
    "@track_execution_time\n",
    "def q1_time(file_path: str, lines_per_chunk = 1000, num_workers = 4) -> List[Tuple[datetime.date, str]]:\n",
    "    \n",
    "    chunks = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            chunk.append(line)\n",
    "            if len(chunk) >= lines_per_chunk:\n",
    "                chunks.append(chunk)\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    results = defaultdict(Counter)\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(process_lines, chunk) for chunk in chunks]\n",
    "        for future in as_completed(futures):\n",
    "            local_counts = future.result()\n",
    "            for date, counter in local_counts.items():\n",
    "                results[date] += counter\n",
    "\n",
    "    final_results = []\n",
    "    for date, counter in results.items():\n",
    "        most_common_user, count = counter.most_common(1)[0]\n",
    "        final_results.append((date, most_common_user, count))\n",
    "\n",
    "    top_results = sorted(final_results, key=lambda x: (-x[2], x[0]))[:10]\n",
    "    return [(date, username) for date, username, _ in top_results]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de función q1_time\n",
    "from q1_time import q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_time executed in 2.806195020675659 seconds.\n",
      "peak memory: 503.52 MiB, increment: 442.39 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "results_q1_time = q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de que teóricamente el objetivo de utilizar este método es optimizar el tiempo de ejecución, ese objetivo no se cumple en este caso. Mi hipótesis para este resultado es que el tamaño del dataset no permite brillar la utilidad del procesamiento paralelo. En la medida que esta tarea se aplique a un dataset de mayor tamaño, es donde el enfoque secuencial comenzará a quedarse atrás en términos de tiempo de ejecución en comparación al enfoque paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 21), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_q1_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segundo desafío"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción y estrategia\n",
    "\n",
    "En el fútbol hay un dicho que dice \"equipo que gana no se toca\", y con esta premisa reutilicé las estrategias que hice en el primer desafío para este segundo. A grandes rasgos, sigo usando la lógica secuencial para maximizar memoria, y el procesamiento paralelo para optimizar tiempo de ejecución.\n",
    "\n",
    "Adicionalmente, para obtener los emojis dentro de los mensajes, tuve que utilizar el paquete [emoji](https://pypi.org/project/emoji/), el cual está pensado precisamente para esta tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendimiento de `q2_memory`\n",
    "\n",
    "La estructura de la función es la siguiente:\n",
    "\n",
    "1. Genera una instancia de contador, utilizando la clase de python Counter (en este caso no necesitamos `defaultdict` porque es un conteo más sencillo).\n",
    "2. Recorre línea por línea el archivo json almacenando en el contador la cantidad de apariciones para cada emoji dentro de la key `content`, que es donde esta el contenido del twit.\n",
    "3. Una vez obtenido, la sumamos un valor a ese elemento dentro del `emoji_counter`. \n",
    "4. Mediante la función `most_common`, obtenemos el emoji más repetido, y lo guardamos en un array `results`.\n",
    "5. Finalmente, retornamos el resultado.\n",
    "\n",
    "```\n",
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            \n",
    "            twit = record['content']\n",
    "            \n",
    "            emojis_list = emoji.emoji_list(twit)\n",
    "            \n",
    "            for emoji_data in emojis_list:\n",
    "                emoji_char = emoji_data['emoji']\n",
    "                emoji_counter[emoji_char] += 1\n",
    "\n",
    "    results = emoji_counter.most_common(10)\n",
    "    \n",
    "    return results\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de función q2_memory\n",
    "from q2_memory import q2_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q2_memory executed in 7.581460952758789 seconds.\n",
      "peak memory: 78.70 MiB, increment: 0.38 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit # Usamos este magic command para aplicar el memory_profiler en esta celda\n",
    "\n",
    "results_q2_memory = q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede apreciarse, el tiempo de ejecución es de 7.5 segundos, generando un incremento de 0.38 megas de ram, ubicándonos en un pico de 78.70 en total. A continuación, los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_q2_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendimiento de `q2_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como mencioné previamente, en esta instancia se volvió a utilizar la lógica de procesamiento paralelo para optimizar el tiempo de ejecución a costa del uso de memoria. Tiene una estructura muy similar a `q1_time`.\n",
    "\n",
    "La estructura de la función es la siguiente:\n",
    "1. Definimos `process_lines`, que básicamente realiza el trabajo de recolectar los emojis de cada contenido de twits, y guardarlo en el contador general de emojis.\n",
    "2. Generamos una serie de chunks sobre el dataset original, de modo de procesar esta información paralelamente más adelante.\n",
    "3. Generamos una instancia de `ThreadPoolExecutor`, en donde se aplicará la función `process_lines` a cada chunk creado\n",
    "4. En la medida que los chunks se van procesando, se actualizan los valores del `total_emoji_counter`. Al ser un conteo simple, no necesitamos usar un diccionario más complejo.\n",
    "5. Mediante la función `most_common`, obtenemos los emojis con valores más altos y los guardamos en un array `results`.\n",
    "6. Finalmente, retornamos el resultado.\n",
    "\n",
    "```\n",
    "def process_chunk(chunk):\n",
    "    chunk_emoji_counter = Counter()\n",
    "    \n",
    "    for line in chunk:\n",
    "        record = json.loads(line)\n",
    "        twit = record['content']\n",
    "        emojis_list = emoji.emoji_list(twit)\n",
    "        for emoji_data in emojis_list:\n",
    "            emoji_char = emoji_data['emoji']\n",
    "            chunk_emoji_counter[emoji_char] += 1\n",
    "    \n",
    "    return chunk_emoji_counter\n",
    "\n",
    "@track_execution_time\n",
    "def q2_time(file_path: str, num_workers: int = 4, lines_per_chunk = 10000) -> List[Tuple[str, int]]:\n",
    "    chunks = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            chunk.append(line)\n",
    "            if len(chunk) >= lines_per_chunk:\n",
    "                chunks.append(chunk)\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    total_emoji_counter = Counter()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "        for future in as_completed(futures):\n",
    "            chunk_emoji_counter = future.result()\n",
    "            total_emoji_counter.update(chunk_emoji_counter)\n",
    "    \n",
    "    results = total_emoji_counter.most_common(10)\n",
    "    \n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de función q2_time\n",
    "from q2_time import q2_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q2_time executed in 7.839390993118286 seconds.\n",
      "peak memory: 478.83 MiB, increment: 360.77 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "results_q2_time = q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como ocurre en el primer desafío, no vemos una mejora en términos de tiempo, mas si un aumento del uso de memoria. Se mantiene mi hipótesis de que a mayor escala se podrían ver diferencias en este sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_q2_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tercer desafío"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción y estrategia\n",
    "\n",
    "El enfoque de este desafío es similar al `q2` en la medida que consta de un conteo simple dentro de los registros. Para optimizar la memoria, se optó por una lógica secuencial, y para una optimización del tiempo, por una lógica de procesamiento paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendimiento de `q3_memory`\n",
    "\n",
    "La estructura de la función es la siguiente:\n",
    "\n",
    "1. Genera una instancia de contador, utilizando la clase de python Counter.\n",
    "2. Recorre línea por línea el archivo json buscando el objeto `mentionedUsers`, en donde, si es que existe, itera almacenando en el contador la cantidad de apariciones para cada usuario. Nos quedamos con `username` ya que es el usuario único de la persona, no su display name.\n",
    "3. Una vez obtenido, la sumamos un valor a ese elemento dentro del `users_counter`. \n",
    "4. Mediante la función `most_common`, obtenemos los 10 usuarios más repetidos, y lo guardamos en un array `results`.\n",
    "5. Finalmente, retornamos el resultado.\n",
    "\n",
    "```\n",
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    users_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            \n",
    "            users = record['mentionedUsers']\n",
    "            \n",
    "            if users:\n",
    "                for user in users:\n",
    "                    username = user['username']\n",
    "                    users_counter[username] += 1\n",
    "\n",
    "    results = users_counter.most_common(10)\n",
    "\n",
    "    return results\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de función q2_memory\n",
    "from q3_memory import q3_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q3_memory executed in 2.0018150806427 seconds.\n",
      "peak memory: 69.67 MiB, increment: 1.67 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit # Usamos este magic command para aplicar el memory_profiler en esta celda\n",
    "\n",
    "results_q3_memory = q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede apreciarse, el tiempo de ejecución es de 2 segundos, generando un incremento de 1.67 megas de ram, ubicándonos en un pico de 69.67 en total. A continuación, los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_q3_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendimiento de `q3_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como mencioné previamente, en esta instancia se volvió a utilizar la lógica de procesamiento paralelo para optimizar el tiempo de ejecución a costa del uso de memoria. Tiene una estructura muy similar a `q1_time` y `q2_time`.\n",
    "\n",
    "La estructura de la función es la siguiente:\n",
    "1. Definimos `process_lines`, que básicamente realiza el trabajo de recolectar los usuarios mencionados de cada twit, y guardarlo en el contador general de usuarios.\n",
    "2. Generamos una serie de chunks sobre el dataset original, de modo de procesar esta información paralelamente más adelante.\n",
    "3. Generamos una instancia de `ThreadPoolExecutor`, en donde se aplicará la función `process_lines` a cada chunk creado\n",
    "4. En la medida que los chunks se van procesando, se actualizan los valores del `total_users_counter`. Al ser un conteo simple, no necesitamos usar un diccionario más complejo.\n",
    "5. Mediante la función `most_common`, obtenemos los usuarios con valores más altos y los guardamos en un array `results`.\n",
    "6. Finalmente, retornamos el resultado.\n",
    "\n",
    "```\n",
    "def process_chunk(chunk):\n",
    "    chunk_users_counter = Counter()\n",
    "    \n",
    "    for line in chunk:\n",
    "        record = json.loads(line)\n",
    "        users = record['mentionedUsers']\n",
    "        if users:\n",
    "            for user in users:\n",
    "                username = user['username']\n",
    "                chunk_users_counter[username] += 1\n",
    "    \n",
    "    return chunk_users_counter\n",
    "\n",
    "@track_execution_time\n",
    "def q3_time(file_path: str, num_workers: int = 4, lines_per_chunk = 10000) -> List[Tuple[str, int]]:\n",
    "    chunks = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            chunk.append(line)\n",
    "            if len(chunk) >= lines_per_chunk:\n",
    "                chunks.append(chunk)\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    total_users_counter = Counter()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "        for future in as_completed(futures):\n",
    "            chunk_users_counter = future.result()\n",
    "            total_users_counter.update(chunk_users_counter)\n",
    "    \n",
    "    results = total_users_counter.most_common(10)\n",
    "    \n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de función q3_time\n",
    "from q3_time import q3_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q3_time executed in 2.3009002208709717 seconds.\n",
      "peak memory: 493.11 MiB, increment: 427.03 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "results_q3_time = q3_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como ocurre en el primer desafío, no vemos una mejora en términos de tiempo, mas si un aumento del uso de memoria. Se mantiene mi hipótesis de que a mayor escala se podrían ver diferencias en este sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_q3_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
